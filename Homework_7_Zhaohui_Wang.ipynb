{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2hPDx3Tvh0v"
      },
      "source": [
        "# Homework 7\n",
        "\n",
        "In this homework you will be training and using a \"char-RNN\". This is the name given to a character-level recurrent neural network language model by [this famous blog post by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Before you start on the rest of the homework, please give the blog post a read, it's quite good!\n",
        "\n",
        "I don't expect you to implement the char-RNN from scratch. Andrej's original char-rnn is in Torch (the predecessor to PyTorch that is not commonly used anymore). Fortunately, there are many other implementations of this model available; for example, there is one (in both mxnet and pytorch) in chapters 8 and 9 of [the textbook](http://d2l.ai), and another pytorch one [here](https://github.com/spro/char-rnn.pytorch). **Please use one of these example implementations (or another one that you find) when completing this homework**.\n",
        "\n",
        "For this homework, please complete the following steps:\n",
        "\n",
        "1. Download and tokenize the [Shakespeare dataset](http://www.gutenberg.org/files/100/100-0.txt) at a character level. I recommend basing your solution on the following code:\n",
        "```Python\n",
        "# Remove non-alphabetical characters, lowercase, and replace whitespace with ' '\n",
        "raw_dataset = ' '.join(re.sub('[^A-Za-z ]+', '', text).lower().split())\n",
        "# Maps token index to character\n",
        "idx_to_char = list(set(raw_dataset))\n",
        "# Maps character to token index\n",
        "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
        "# Tokenize the dataset\n",
        "corpus_indices = [char_to_idx[char] for char in raw_dataset]\n",
        "```\n",
        "1. Train a \"vanilla\" RNN (as described in chapter 8 of [the textbook](http://d2l.ai)) on the Shakespeare dataset. Report the training loss and generate some samples from the model at the end of training.\n",
        "1. Train a GRU RNN (as described in chapter 9 of [the textbook](http://d2l.ai)) on the Shakespeare datatset. Is the final training loss higher or lower than the vanilla RNN? Are the samples from the model more or less realistic?\n",
        "1. Find a smaller, simpler dataset than the Shakespeare data (you can find some ideas in Andrej's blog post, but feel free to get creative!) and train either the vanilla or GRU RNN on it instead. Is the final training loss higher or lower than it was for the Shakespeare data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQDaBydsKWlg"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "The results of the Vanilla RNN model on the Shakespeare dataset are as follows:\n",
        "\n",
        "![vrnn](https://github.com/ZhaohuiWang-github/COMP664-deep-learning/raw/main/hw7_data/Vanilla%20RNN%20Training%20Loss.png)\n",
        "\n",
        "- The time spent on training was **3min 37s**.\n",
        "\n",
        "- We can see that the fluctuation of the loss is very dramatic during the training process, and after 2000 epochs, the loss is reduced to about **1.9**.\n",
        "\n",
        "Then, I set the start character to `wh` and the length of the generated samples to 200, and got 3 samples as follows:\n",
        "\n",
        "```\n",
        "Sample 1:  whoun's bolns swee. be do me, reng\n",
        "in thee to then new, he all it, and in and am are must the samess a sife why, thou are the,\n",
        "thy mill lock. whis.\n",
        "         thee.\n",
        "\n",
        "                                 \n",
        "\n",
        "Sample 2:  whe kne, still gurains, and every we she of youpal look in you, o, the came of that he roperhers lew we stake my lo come i be harthen ever the cith and be ne a buicing heverss but surn-whicine. thy clam\n",
        "\n",
        "Sample 3:  when it; good me bed\n",
        "                      136_eas me stur somes, my shale will counter a do way i he a she my swee. and nnor thee you best hemser befor cork\n",
        "i ato and herad coment the wom no, a speep\n",
        "\n",
        "```\n",
        "\n",
        "- We can find that the generated samples are not very good. There are a large number of words that are wrong, especially words that are slightly longer in length. The syntax of these sentences is also very bad, and the semantics are so poor that it is almost impossible to see what they are trying to express.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfeDG2hgTmhS"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "The results of the GRU RNN model on the Shakespeare dataset are as follows:\n",
        "\n",
        "![grurnn](https://github.com/ZhaohuiWang-github/COMP664-deep-learning/raw/main/hw7_data/GRU%20RNN%20Training%20Loss.png)\n",
        "\n",
        "- The time spent on training was **5min 24s**. \n",
        "- The training time of GRU RNN is significantly longer than that of  Vanilla RNN, since GRU RNN has a more complex structure and more parameters.\n",
        "\n",
        "- We can see that the fluctuation of the loss is also very dramatic during the training process. After 2000 epochs, the loss is reduced to about **1.8**.\n",
        "- Relatively speaking, the loss of GRU RNN is slightly smaller than that of Vanilla RNN, but the difference is not very significant.\n",
        "\n",
        "Similarly, I set the start character to `wh` and the length of the generated samples to 200, and got 3 samples as follows:\n",
        "\n",
        "```\n",
        "Sample 1:  whee a my the wordiaven\n",
        "           arither? this that truce-days.\n",
        "happs,\n",
        "and as the for worrether can my faighter,\n",
        "the secnaom dewet these but, so as for the will mard:\n",
        "       sir one, a thou vieli\n",
        "\n",
        "Sample 2:  whered hath i some frowned,\n",
        "             wind this been the drus, thou fair this crove am, it so mearn any not hath man\n",
        "    no she lord he wind? no mundace. the a copless,\n",
        "\n",
        "   that him somed the she\n",
        "\n",
        "Sample 3:  whis his this that by he manad, happ,\n",
        "but\n",
        "     thy duke i courn with forther, 'twordanded?\n",
        "such suing empertaur but see of the agencar, no the shall that viled\n",
        "    marted dischty my chary well consh\n",
        "\n",
        "```\n",
        "\n",
        "- We can find that the three generated samples are not very good either, with many wrong words, syntax errors, and unclear semantics. I was expecting more realistic samples generated by the GRU model, but found that it was not obvious.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlXiy45YY-Q6"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "The smaller, simpler dataset I chose is from [Pytorch Tutorials](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html), and I only used the [`Chinese.txt`](https://github.com/ZhaohuiWang-github/COMP664-deep-learning/blob/main/hw7_data/Chinese.txt) in it. This dataset records some Chinese names, and it has only 1247 total characters. And the names themselves are also very short. \n",
        "\n",
        "The results of the Vanilla RNN model on the Chinese Name dataset are as follows:\n",
        "\n",
        "![vrnn](https://github.com/ZhaohuiWang-github/COMP664-deep-learning/raw/main/hw7_data/Vanilla%20RNN%20Training%20Loss%20of%20Chinese%20Names.png)\n",
        "\n",
        "- The time spent on training was **3min 51s**.\n",
        "- We can see that the fluctuations of loss on the Chinese Name dataset are significantly smaller than those on the Shakespeare dataset. After 2000 epochs, the loss is reduced to about **0.07**, which is lower than it was for the Shakespeare data.\n",
        "\n",
        "Then, I set the start character to `na` and the length of the generated samples to 12, and got 3 samples as follows:\n",
        "\n",
        "```\n",
        "Sample 1: \n",
        "nao\n",
        "shao\n",
        "shaw\n",
        "\n",
        "\n",
        "Sample 2: \n",
        "nao\n",
        "chen\n",
        "cheng\n",
        "\n",
        "Sample 3: \n",
        "na\n",
        "hiu\n",
        "hong\n",
        "ho\n",
        "\n",
        "```\n",
        "\n",
        "- We can find more than half of the samples look like Chinese names.\n",
        "- The model performs significantly better on the Chinese Name dataset than on the Shakespeare dataset, regardless of the loss values or the degree of reality of the generated samples. This may be due to the fact that the names themselves are simpler than sentences or paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT3RnBIwgDZj"
      },
      "source": [
        "**Conclusion:** \n",
        "\n",
        "The results of the GRU RNN model on the Chinese Name dataset are as follows:\n",
        "\n",
        "![grurnn](https://github.com/ZhaohuiWang-github/COMP664-deep-learning/raw/main/hw7_data/GRU%20RNN%20Training%20Loss%20of%20Chinese%20Names.png)\n",
        "\n",
        "- The time spent on training was **5min 29s**. \n",
        "- Similarly, the training time of GRU RNN is significantly longer than that of Vanilla RNN, since GRU RNN has a more complex structure and more parameters.\n",
        "\n",
        "- Obviously, the loss curve of the GRU RNN model is better and smoother than that of the Vanilla RNN model on the Chinese Name dataset. After 2000 epochs, the loss is reduced to about **0.04**, which is lower than it was for the Vanilla RNN model, and it is lower than it was for the Shakespeare data.\n",
        "\n",
        "Similarly, I set the start character to `na` and the length of the generated samples to 12, and got 3 samples as follows:\n",
        "\n",
        "```\n",
        "Sample 1: \n",
        "nang\n",
        "luo\n",
        "mah\n",
        "m\n",
        "\n",
        "Sample 2: \n",
        "nang\n",
        "zhao\n",
        "zhen\n",
        "\n",
        "Sample 3: \n",
        "nang\n",
        "jin\n",
        "jing\n",
        "```\n",
        "\n",
        "- There are also more than half of the samples that look like Chinese names.\n",
        "- Therefore, whether it is the Vanilla RNN or the GRU RNN, the performance on the Chinese Name dataset is significantly better than the performance on the Shakespeare dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfDRJkFYNRoc"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puOFLdb-uH95"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjTL-5BqaVz9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Shakespeare dataset\n",
        "# filename = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "\n",
        "# Chinese Names dataset\n",
        "filename = \"https://github.com/ZhaohuiWang-github/COMP664-deep-learning/raw/main/hw7_data/Chinese.txt\"\n",
        "\n",
        "file = requests.get(filename)\n",
        "file = file.text\n",
        "\n",
        "# Or read local file\n",
        "# file = open('shakespeare.txt').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAP84bsdfleg",
        "outputId": "02b8d766-6abb-4a1c-e716-0613d3f73e0f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.8/dist-packages (1.3.6)\n"
          ]
        }
      ],
      "source": [
        "# Remove non-alphabetical characters, lowercase, and replace whitespace with ' '\n",
        "# file = ' '.join(re.sub('[^A-Za-z ]+', '', file).lower().split())\n",
        "# file = ''.join(re.sub('[^A-Za-z ]+', '', file).lower().split())\n",
        "\n",
        "\n",
        "# I find that unidecode works better\n",
        "!pip install Unidecode\n",
        "import unidecode\n",
        "file = unidecode.unidecode(file).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aHqxcigxvgcD",
        "outputId": "f43bf0f8-9a1d-4c51-f1da-13b925966170",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'kzlcas-:jiboqrxynpugedhtw\\n mf'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# all different charcters in the dataset\n",
        "all_characters = ''.join(set(file))\n",
        "all_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SY1Mr5UtG1x",
        "outputId": "8482c98c-8d9f-4b77-8900-4fa524765167",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number of different characters in the dataset\n",
        "n_characters = len(set(file))\n",
        "n_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4vNSn2ytYE5",
        "outputId": "e580dd0a-b1cc-4677-d977-fa85c9f657a1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1247"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# length of the dataset\n",
        "file_len = len(file)\n",
        "file_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgtJEY4ntaP8",
        "outputId": "ee7cd88a-5446-480e-aaf2-713dff1cdf57",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e \n",
            "seow\n",
            "seto\n",
            "sha\n",
            "shan\n",
            "shang\n",
            "shao\n",
            "shaw\n",
            "she\n",
            "shen\n",
            "sheng\n",
            "shi\n",
            "shu\n",
            "shuai\n",
            "shui\n",
            "shum\n",
            "siew\n",
            "siu\n",
            "song\n",
            "sum\n",
            "sun\n",
            "sze \n",
            "tan\n",
            "tang\n",
            "tao\n",
            "teng\n",
            "teoh\n",
            "thean\n",
            "thian\n",
            "thien\n",
            "tian\n",
            "tong\n",
            "tow\n",
            "tsang\n",
            "tse\n",
            "tsen\n",
            "tso\n",
            "tze\n",
            "wan\n",
            "wang\n",
            "wei\n",
            "wen\n",
            "we\n"
          ]
        }
      ],
      "source": [
        "# To make inputs out of this big string of data, we will be splitting it into chunks\n",
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_xi8SlqvPJD",
        "outputId": "0b21d827-edeb-4840-ccdc-a3585ace9853",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 4, 10,  3, 21])\n"
          ]
        }
      ],
      "source": [
        "# Turn string into list of token index\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor)\n",
        "\n",
        "print(char_tensor('abcd'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UytpcE1LxQi5",
        "outputId": "af08c984-206d-4739-a7ca-59a8032a8e3b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([19, 25,  3, 22, 20, 24, 25,  3, 22,  9, 20, 18, 25,  3, 22,  9, 16, 25,\n",
              "          3, 22, 11, 16, 19, 25,  3, 22, 11, 18, 25,  3, 22, 18, 25,  3, 18,  9,\n",
              "         25, 21,  4,  9, 25, 21, 20, 16, 19, 25, 21,  9, 16, 19, 25, 21, 11, 16,\n",
              "         19, 25, 21, 11, 18, 25, 21, 18,  4, 16, 25, 20, 16, 19, 25, 28,  4, 16,\n",
              "         25, 28, 20,  9, 25, 28, 20, 16, 19, 25, 28, 11, 11, 16, 19, 25, 28, 18,\n",
              "         16, 19, 25, 19,  4, 16, 25, 19,  4, 18,  0, 25, 19, 20, 16, 19, 25, 19,\n",
              "          9, 27, 25, 19, 11,  0, 25, 19, 11, 16, 19, 25, 19, 18,  4, 16, 25, 19,\n",
              "         18,  4, 16, 19, 25, 19, 18, 11, 25, 19, 24, 11,  3,  0, 25, 22,  4, 16,\n",
              "         25, 22,  4, 16, 19, 25, 22,  4, 11, 25, 22, 20, 24, 25, 22,  9, 18, 25,\n",
              "         22, 11, 16, 19, 25, 22, 11, 13, 25, 22,  5,  9,  4, 11, 25, 22, 18,  4,\n",
              "         25, 22, 18,  4, 16, 25, 22, 18,  4, 16, 19, 25, 22, 18,  9, 25, 22, 18,\n",
              "          9, 20]),\n",
              " tensor([25,  3, 22, 20, 24, 25,  3, 22,  9, 20, 18, 25,  3, 22,  9, 16, 25,  3,\n",
              "         22, 11, 16, 19, 25,  3, 22, 11, 18, 25,  3, 22, 18, 25,  3, 18,  9, 25,\n",
              "         21,  4,  9, 25, 21, 20, 16, 19, 25, 21,  9, 16, 19, 25, 21, 11, 16, 19,\n",
              "         25, 21, 11, 18, 25, 21, 18,  4, 16, 25, 20, 16, 19, 25, 28,  4, 16, 25,\n",
              "         28, 20,  9, 25, 28, 20, 16, 19, 25, 28, 11, 11, 16, 19, 25, 28, 18, 16,\n",
              "         19, 25, 19,  4, 16, 25, 19,  4, 18,  0, 25, 19, 20, 16, 19, 25, 19,  9,\n",
              "         27, 25, 19, 11,  0, 25, 19, 11, 16, 19, 25, 19, 18,  4, 16, 25, 19, 18,\n",
              "          4, 16, 19, 25, 19, 18, 11, 25, 19, 24, 11,  3,  0, 25, 22,  4, 16, 25,\n",
              "         22,  4, 16, 19, 25, 22,  4, 11, 25, 22, 20, 24, 25, 22,  9, 18, 25, 22,\n",
              "         11, 16, 19, 25, 22, 11, 13, 25, 22,  5,  9,  4, 11, 25, 22, 18,  4, 25,\n",
              "         22, 18,  4, 16, 25, 22, 18,  4, 16, 19, 25, 22, 18,  9, 25, 22, 18,  9,\n",
              "         20, 25]))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assemble a pair of input and target tensors for training from a random chunk\n",
        "def random_training_set():    \n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target\n",
        "\n",
        "random_training_set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o42mqPbOuLkR"
      },
      "source": [
        "### Build the RNN Model\n",
        "\n",
        "This model will take as input the character for step $t_{-1}$ and is expected to output the next character $t$. There are three layers - one linear layer that encodes the input character into an internal state, one vanilla RNN layer or GRU RNN layer that operates on that internal state and a hidden state, and a decoder layer that outputs the probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7DCr_gPuCK2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Vanilla RNN or GRU RNN\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # ------------------ GRU RNN ------------------\n",
        "        # GRU RNN\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        \n",
        "        # ------------------ Vanilla RNN --------------\n",
        "        # Vanilla RNN\n",
        "        # self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
        "\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "\n",
        "        # ------------------ GRU RNN ------------------\n",
        "        # GRU RNN\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "\n",
        "        # ------------------ Vanilla RNN --------------\n",
        "        # Vanilla RNN\n",
        "        # output, hidden = self.rnn(input.view(1, 1, -1), hidden)\n",
        "\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt0EIGFK1QZz"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDMN5Wwi1kcP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# define the training parameters\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        "\n",
        "# instantiate the model\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caPe4GS701jt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# A helper to print the amount of time passed\n",
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH5NNTVD1T8F",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    target = target.reshape(-1, 1)\n",
        "\n",
        "    for c in range(chunk_len-1):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c])\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # print(loss.item())\n",
        "    return loss.item() / chunk_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrtsJR9b18Uu",
        "outputId": "a1910797-90e7-4005-9a8e-3bc601d4c275",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0m 26s (100 5%) 0.7024\n",
            "0m 43s (200 10%) 0.2602\n",
            "1m 0s (300 15%) 0.1318\n",
            "1m 15s (400 20%) 0.0492\n",
            "1m 31s (500 25%) 0.0328\n",
            "1m 47s (600 30%) 0.0644\n",
            "2m 6s (700 35%) 0.0454\n",
            "2m 22s (800 40%) 0.0268\n",
            "2m 38s (900 45%) 0.0274\n",
            "2m 54s (1000 50%) 0.0484\n",
            "3m 11s (1100 55%) 0.0425\n",
            "3m 26s (1200 60%) 0.0744\n",
            "3m 42s (1300 65%) 0.0301\n",
            "3m 59s (1400 70%) 0.0416\n",
            "4m 16s (1500 75%) 0.0528\n",
            "4m 31s (1600 80%) 0.0366\n",
            "4m 47s (1700 85%) 0.0236\n",
            "5m 4s (1800 90%) 0.0276\n",
            "5m 20s (1900 95%) 0.0332\n",
            "5m 36s (2000 100%) 0.0407\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "n_epochs = 2000\n",
        "print_every = 100\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss = train(*random_training_set())\n",
        "    \n",
        "    if epoch % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "    \n",
        "    all_losses.append(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "qN75WOY73Du1",
        "outputId": "354d7fad-a9f8-4a6b-a71c-6b3564fbcafc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('GRU RNN Training Loss of Chinese Names Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFNiKLrLqGYa"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfe_i3Tk0CCB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "def predict(prime_str='a', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK-WofZWj3WH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Generate samples for Shakespeare dataset\n",
        "# for i in range(3):\n",
        "#     print('Sample %d: ' %(i+1), predict('wh', 200))\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPTFeMp6yAlQ",
        "outputId": "d3c1e049-9b11-46cf-85a4-52d9f25a22b4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1: \n",
            "nang\n",
            "luo\n",
            "mah\n",
            "m\n",
            "\n",
            "Sample 2: \n",
            "nang\n",
            "zhao\n",
            "zhen\n",
            "\n",
            "Sample 3: \n",
            "nang\n",
            "jin\n",
            "jing\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate samples for Chinese Names dataset\n",
        "for i in range(3):\n",
        "    print('Sample %d: ' %(i+1))\n",
        "    print(predict('na', 12))\n",
        "    print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**References:**\n",
        "- [Practical PyTorch: Generating Shakespeare with a Character-Level RNN](https://github.com/spro/practical-pytorch/tree/master/char-rnn-generation)\n",
        "- [NLP From Scratch: Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
